from copy import deepcopy
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import torch
from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments
import math
import evaluate
import numpy as np
from sklearn.metrics import f1_score, accuracy_score

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BATCH_SIZE = 8

lang = 'en'
type_to_label = { 
    "non_malware": 0,
    "malware": 1,
    "Banking-Malware": 2,  # Was 3
    "Botnets": 3,  # Was 4
    "Browser Hijackers": 4,  # Was 5
    "Email-Worm": 5,  # Was 6
    "Joke": 6,  # Was 7
    "Net-Worm": 7,  # Was 8
    "Pony": 8,  # Was 9
    "Ransomware": 9,  # Was 10
    "RAT": 10,  # Was 11
    "rogues": 11,  # Was 12
    "Spyware": 12,  # Was 13
    "Stealer": 13,  # Was 14
    "Trojan": 14,  # Was 15
    "Virus": 15,  # Was 16
    "Worm": 16  # Was 17
}
num_labels = len(type_to_label)

metric = evaluate.load("accuracy")

class MalwareDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels

    def __len__(self):
        return len(self.sequences['input_ids'])

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.sequences.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Change to long for CrossEntropy
        return item
    
### Custom callback to evaluate the transformer (updated for multiclass)

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)  # Get the predicted class index (instead of sigmoid for multilabel)
    labels = p.label_ids

    # Calculate accuracy and f1 score
    accuracy = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average='micro')  # You can use 'macro' or 'weighted' depending on your needs

    metrics = {
        'accuracy': accuracy,
        'f1': f1,
    }
    return metrics

# ---------------------------------------------------------

def split_mnemonic_codes_df(data_frame):
    # No need for one-hot encoding, return labels as integers
    data_frame['type'] = data_frame['type'].apply(lambda x: type_to_label[x])
    
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        data_frame['opcodes'].to_list(), 
        data_frame['type'].to_list(), 
        test_size=0.2, 
        random_state=1337,
        shuffle=True
    )
    
    return train_texts, val_texts, train_labels, val_labels

def train_model_with_mnemonic_codes(train_texts, val_texts, train_labels, val_labels):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    tokenized_train_sequences = tokenizer(train_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    train_dataset = MalwareDataset(tokenized_train_sequences, train_labels)

    tokenized_eval_sequences = tokenizer(val_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    eval_dataset = MalwareDataset(tokenized_eval_sequences, val_labels)

    # Use the default problem type (no need to specify multi-label classification)
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels).to(device)

    training_args = TrainingArguments(
        evaluation_strategy="epoch",
        save_strategy="epoch",
        output_dir="./bert_multiclass_classification",
        num_train_epochs=20,  
        per_device_train_batch_size=BATCH_SIZE,  
        per_device_eval_batch_size=BATCH_SIZE,
        weight_decay=0.01,   
        load_best_model_at_end=True,    
        metric_for_best_model="f1",
        logging_steps=2,
    )

    trainer = Trainer(
        model=model,                         
        args=training_args,                  
        train_dataset=train_dataset,    
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()
    trainer.evaluate()

    return model

def __classify_the_sequences(sequence, model, tokenizer):
    tokenized_sequences = tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
    outputs = model(**tokenized_sequences)
    predictions = torch.argmax(outputs.logits, dim=1)  # Get the predicted class index
    return predictions.cpu().numpy()

def evaluate_model_with_mnemonic_codes(model, train_texts, labels_val):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    total_correct = 0
    total_test_sets = 0

    for idx, val in enumerate(train_texts):
        prediction = __classify_the_sequences(val, model, tokenizer)
        
        # Calculate the accuracy per sequence
        total_correct += (prediction == labels_val[idx])
        total_test_sets += 1

    accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
    print(f"Total accuracy: {accuracy}")
