from copy import deepcopy
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import torch
from transformers import BertForSequenceClassification, BertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import math
import evaluate
import numpy as np


from sklearn.metrics import f1_score, roc_auc_score, accuracy_score

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BATCH_SIZE = 8;

lang = 'en'
type_to_label = { 
    "non_malware": 0,
    "malware": 1,
    "Banking-Malware": 3,
    "Botnets": 4,
    "Browser Hijackers": 5,
    "Email-Worm": 6,
    "Joke": 7,
    "Net-Worm": 8,
    "Pony": 9,
    "Ransomware": 10,
    "RAT": 11,
    "rogues": 12,
    "Spyware": 13,
    "Stealer": 14,
    "Trojan": 15,
    "Virus": 16,
    "Worm": 17,
}

num_labels = len(type_to_label)

metric = evaluate.load("accuracy")

class MalwareDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels

    def __len__(self):
        return len(self.sequences['input_ids'])

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.sequences.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float) 
        return item
    
### Custom callback to evaluate the transformer
    
def multilabel_metrics(predictions, labels, threshold=0.5):
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.Tensor(predictions))
    y_pred = np.zeros(probs.shape)
    y_pred[np.where(probs >= threshold)] = 1
    
    y_true = labels

    # Use multilabel versions of metrics
    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')
    roc_auc = roc_auc_score(y_true, y_pred, average='micro')
    
    accuracy = accuracy_score(y_true, y_pred)

    metrics = {
        'f1': f1_micro_average,
        'roc_auc': roc_auc,
        'accuracy': accuracy
    }
    return metrics

def compute_metrics(p):
    preds = p.predictions[0] if isinstance(p.predictions, 
            tuple) else p.predictions
    result = multilabel_metrics(
        predictions=preds, 
        labels=p.label_ids)
    return result

# ---------------------------------------------------------

def split_mnemonic_codes_df(data_frame):
    data_frame['type'] = data_frame['type'].apply(lambda x: [1 if i == type_to_label[x] else 0 for i in range(num_labels)])
    
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        data_frame['opcodes'].to_list(), 
        data_frame['type'].to_list(), 
        test_size=0.2, 
        random_state=1337,
        shuffle=True
    )
    return train_texts, val_texts, train_labels, val_labels

def train_model_with_mnemonic_codes(train_texts, val_texts, train_labels, val_labels):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    tokenized_train_sequences = tokenizer(train_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    train_dataset = MalwareDataset(tokenized_train_sequences, train_labels)

    tokenized_eval_sequences = tokenizer(val_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    eval_dataset = MalwareDataset(tokenized_eval_sequences, val_labels)

    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels).to(device)

    training_args = TrainingArguments(
        evaluation_strategy="epoch",
        save_strategy="epoch",
        output_dir="./bert_multilabel_classification",
        num_train_epochs=5,  
        learning_rate=2e-5,            
        per_device_train_batch_size=BATCH_SIZE,  
        per_device_eval_batch_size=BATCH_SIZE,
        weight_decay=0.01,   
        load_best_model_at_end=True,    
        metric_for_best_model="f1",
        logging_steps=2,
    )

    trainer = Trainer(
        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,    
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()
    trainer.evaluate()

    return model

def __classify_the_sequences(sequence, model, tokenizer):
    tokenized_sequences = tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
    outputs = model(**tokenized_sequences)
    predictions = torch.sigmoid(outputs.logits)
    return (predictions >= 0.5).float()  

def evaluate_model_with_mnemonic_codes(model, train_texts, labels_val):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    total_correct = 0
    total_test_sets = 0

    for idx, val in enumerate(train_texts):
        prediction =  __classify_the_sequences(val, model, tokenizer).cpu().numpy()
        
        # Calculate the accuracy per sequence
        total_correct += np.all(prediction == np.array(labels_val[idx]))
        total_test_sets += 1

    accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
    print(f"Total accuracy: {accuracy}")
