from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import evaluate
import numpy as np

import csv
import os

 # Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_labels = 2

lang = 'en'

type_to_label = { 
    "non_malware": 0,
    "malware": 1
}

label_to_type = {
    0: "non_malware",
    1: "malware"
}

metric = evaluate.load("accuracy")

class MalwareDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels
    def __len__(self):
        return len(self.sequences['input_ids'])
    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.sequences.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item
  
### Custom callback to evaluate the transformer
  
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
# ---------------------------------------------------------


def split_mnemonic_codes_df(data_frame):
    train_texts, val_texts, train_labels, val_labels, train_filenames, val_filenames = train_test_split(
        data_frame['opcodes'].to_list(), 
        data_frame['type'].map(type_to_label).values, 
        data_frame['file_name'].to_list(), 
        test_size=0.2, 
        random_state=1337,
        shuffle=True
    )
    
    return train_texts, val_texts, train_labels, val_labels, train_filenames, val_filenames



def train_model_with_mnemonic_codes(train_texts, val_texts, train_labels, val_labels):
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    tokenized_train_sequences = tokenizer(train_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    train_dataset = MalwareDataset(tokenized_train_sequences, train_labels)
    
    tokenized_eval_sequences = tokenizer(val_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    eval_dataset = MalwareDataset(tokenized_eval_sequences, val_labels)
    
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("BEFORE TRAINING")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    
    
    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels).to(device)
    
    training_args = TrainingArguments(
        evaluation_strategy="epoch",
        output_dir='./binary_bert_classification',         
        num_train_epochs=5,              
        per_device_train_batch_size=8,  
        per_device_eval_batch_size=8,
        warmup_steps=100,      
        weight_decay=0.1,               
        logging_dir='./logs',            
        logging_steps=2,
    )
    
    trainer = Trainer(
        model=model,                         # the instantiated ü§ó Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,    
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics
    )
    
    trainer.train()
    trainer.evaluate()
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("AFTER TRAIN")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    return model

def __classify_the_sequences(sequence, model, tokenizer):
    tokenized_sequences = tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
    outputs = model(**tokenized_sequences)
    
    predictions = torch.argmax(outputs.logits, dim=1)
    malware_prob = torch.sigmoid(outputs.logits)[:, -1].item()
    return predictions, malware_prob

def __get_malware_probability_directory():
    sourceFileDir = os.path.dirname(os.path.abspath(__file__))
    filepath = os.path.join(sourceFileDir, "malware_probability.csv")
    return filepath

def __instantiate_malware_probability_header():
    with open(__get_malware_probability_directory(), "w", newline="") as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(["file_name", "mnemonic_opcode", "correct_label", "predicted_label", "malware_prob"]) 

def __save_malware_probability(csvfile, file_name, mnemonic, correct_label, predicted_label, malware_prob):
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow([file_name, mnemonic, correct_label, predicted_label, malware_prob]) 

def evaluate_model_with_mnemonic_codes(model, texts_val, labels_val, filenames_val, shuffled_data):
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("INITIALIZING EVALUATION")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    total_correct = 0
    total_test_sets = 0
    accuracy = 0
    
    for idx, mnemonic in enumerate(texts_val):
        prediction, malware_prob =  __classify_the_sequences(mnemonic, model, tokenizer)
        
        correct_label = labels_val[idx]
        print(f"Predicted Label: {prediction.item()}, Correct label: {correct_label}")
        
        total_correct += (prediction.item() == correct_label).sum()
        total_test_sets += 1
        accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
        print(f"trainset accuracy: {accuracy}")
    
    
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("INITIALIZING MALWARE PROBRABILITY CSV CONSTRUCTOR")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    
    
    texts_to_csv = shuffled_data['opcodes'].to_list()
    labels_to_csv = shuffled_data['type'].map(type_to_label).values.tolist()
    filenames_to_csv = shuffled_data['file_name'].to_list()
    
    __instantiate_malware_probability_header()
    with open(__get_malware_probability_directory(), "a", newline="") as csvfile:
        for idx, mnemonic in enumerate(texts_to_csv):
            prediction, malware_prob =  __classify_the_sequences(mnemonic, model, tokenizer)
            
            correct_label = labels_to_csv[idx]
            print(f"Predicted Label: {prediction.item()}, Correct label: {correct_label}")
            
            __save_malware_probability(csvfile, filenames_to_csv[idx], mnemonic, correct_label, prediction.item(), malware_prob )
            
            total_correct += 1 if (prediction.item() == correct_label) else 0
            total_test_sets += 1
            accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
            print(f"trainset accuracy: {accuracy}")
            
    accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
    print(f"Total accuracy: {accuracy}")
    
    return accuracy
    
    
import matplotlib.pyplot as plt

def plot_training_results(trainer, output_dir="./plots"):
    print("CHEGOU EM PLOT TRAINING RESULTS")
    # Pega os logs de treinamento
    logs = trainer.state.log_history

    # Separa os valores de loss e accuracy
    epochs = [log["epoch"] for log in logs if "loss" in log]
    loss_values = [log["loss"] for log in logs if "loss" in log]
    eval_loss_values = [log["eval_loss"] for log in logs if "eval_loss" in log]
    eval_accuracy_values = [log["eval_accuracy"] for log in logs if "eval_accuracy" in log]

    # Plota a loss de treinamento e valida√ß√£o
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, loss_values, label="Training Loss")
    plt.plot(epochs, eval_loss_values, label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss over Epochs")
    plt.legend()
    plt.grid(True)
    plt.savefig(f"{output_dir}/loss_plot.png")
    plt.close()

    # Plota a acur√°cia de valida√ß√£o
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, eval_accuracy_values, label="Validation Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title("Validation Accuracy over Epochs")
    plt.legend()
    plt.grid(True)
    plt.savefig(f"{output_dir}/accuracy_plot.png")
    plt.close()

    print(f"Plots saved to {output_dir}")
    return plt
