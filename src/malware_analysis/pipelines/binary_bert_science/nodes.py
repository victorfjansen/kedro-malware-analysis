from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback
import evaluate
import numpy as np
import matplotlib.pyplot as plt
import random

import csv
import os

 # Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_labels = 2

lang = 'en'

type_to_label = { 
    "non_malware": 0,
    "malware": 1
}

label_to_type = {
    0: "non_malware",
    1: "malware"
}

metric = evaluate.load("accuracy")

SEED = 42

random.seed(SEED)
np.random.seed(SEED)

torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)

torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

class MalwareDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels
    def __len__(self):
        return len(self.sequences['input_ids'])
    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.sequences.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item
    
class MetricsCallback(TrainerCallback):
    def __init__(self):
        self.train_loss = []
        self.eval_loss = []
        self.train_acc = []
        self.eval_acc = []
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            if 'loss' in logs:
                self.train_loss.append(logs['loss'])
            if 'eval_loss' in logs:
                self.eval_loss.append(logs['eval_loss'])
            if 'eval_accuracy' in logs:
                self.eval_acc.append(logs['eval_accuracy'])
            if 'accuracy' in logs:  # 'accuracy' is logged during training by the Trainer
                self.train_acc.append(logs['accuracy'])

  
### Custom callback to evaluate the transformer
  
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
# ---------------------------------------------------------


def split_mnemonic_codes_df(data_frame):
    train_texts, val_texts, train_labels, val_labels, train_filenames, val_filenames = train_test_split(
        data_frame['opcodes'].to_list(), 
        data_frame['type'].map(type_to_label).values, 
        data_frame['file_name'].to_list(), 
        test_size=0.2, 
        random_state=1337,
        shuffle=True
    )
    
    return train_texts, val_texts, train_labels, val_labels, train_filenames, val_filenames



def train_model_with_mnemonic_codes(train_texts, val_texts, train_labels, val_labels):
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    tokenized_train_sequences = tokenizer(train_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    train_dataset = MalwareDataset(tokenized_train_sequences, train_labels)
    
    tokenized_eval_sequences = tokenizer(val_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    eval_dataset = MalwareDataset(tokenized_eval_sequences, val_labels) 
    
    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels).to(device)
    metrics_callback = MetricsCallback()
    
    training_args = TrainingArguments(
        evaluation_strategy="epoch",
        output_dir='./binary_bert_classification',         
        num_train_epochs=10,              
        per_device_train_batch_size=8,  
        per_device_eval_batch_size=8,
        warmup_steps=100,      
        weight_decay=0.01,               
        logging_dir='./logs',            
        logging_steps=2,
    )
    
    trainer = Trainer(
        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,    
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
        callbacks=[metrics_callback]
    )
    
    trainer.train()
    trainer.evaluate()
    return model, metrics_callback

def __classify_the_sequences(sequence, model, tokenizer):
    tokenized_sequences = tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
    outputs = model(**tokenized_sequences)
    
    predictions = torch.argmax(outputs.logits, dim=1)
    malware_prob = torch.sigmoid(outputs.logits)[:, -1].item()
    return predictions, malware_prob

def __get_malware_probability_directory():
    sourceFileDir = os.path.dirname(os.path.abspath(__file__))
    filepath = os.path.join(sourceFileDir, "results/malware_probability.csv")
    return filepath

def __instantiate_malware_probability_header():
    with open(__get_malware_probability_directory(), "w", newline="") as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(["file_name", "mnemonic_opcode", "correct_label", "predicted_label", "malware_prob"]) 

def __save_malware_probability(csvfile, file_name, mnemonic, correct_label, predicted_label, malware_prob):
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow([file_name, mnemonic, correct_label, predicted_label, malware_prob]) 

def evaluate_model_with_mnemonic_codes(model, texts_val, labels_val, filenames_val, shuffled_data):
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("INITIALIZING EVALUATION")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    csv_data = []
    total_correct = 0
    total_test_sets = 0
    accuracy = 0
    
    for idx, mnemonic in enumerate(texts_val):
        prediction, malware_prob =  __classify_the_sequences(mnemonic, model, tokenizer)
        
        correct_label = labels_val[idx]
        total_correct += (prediction.item() == correct_label).sum()
        total_test_sets += 1
        accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
        print(f"trainset accuracy: {accuracy}")
    
    
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("INITIALIZING MALWARE PROBRABILITY CSV CONSTRUCTOR")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
        
    correct_labels = []
    predicted_labels = []
    
    __instantiate_malware_probability_header()
    with open(__get_malware_probability_directory(), "a", newline="") as csvfile:
        for idx, mnemonic in enumerate(texts_val):
            prediction, malware_prob =  __classify_the_sequences(mnemonic, model, tokenizer)
            
            correct_label = labels_val[idx]
            
            # append to build confusion_matrix
            correct_labels.append(correct_label)
            predicted_labels.append(prediction.item())            
            
            __save_malware_probability(csvfile, filenames_val[idx], mnemonic, correct_label, prediction.item(), malware_prob)
            
            # Append the row data to csv_data list
            csv_data.append([filenames_val[idx], mnemonic, correct_label, prediction.item(), malware_prob])
            
            total_correct += 1 if (prediction.item() == correct_label) else 0
            total_test_sets += 1
            accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
            print(f"trainset accuracy: {accuracy}")
            
    accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
    print(f"Total accuracy: {accuracy}")
    
    # Create a DataFrame from the collected data
    dataframe = pd.DataFrame(csv_data, columns=["Filename", "Mnemonic", "Correct Label", "Predicted Label", "Malware Probability"])
    
    return accuracy, dataframe, correct_labels, predicted_labels
    
    
import pandas as pd
import plotly.express as px

def plot_metrics_save_figures(metrics_callback):
    os.makedirs('results', exist_ok=True)
    sourceFileDir = os.path.dirname(os.path.abspath(__file__))
    
    # Create a figure for Accuracy
    eval_plot, ax_acc = plt.subplots(figsize=(12, 5))
    ax_acc.plot(metrics_callback.eval_acc, label='Validation accuracy')
    ax_acc.plot(metrics_callback.eval_loss, label='Validation loss')
    ax_acc.set_title('Validation Accuracy')
    ax_acc.set_xlabel('Epochs')
    ax_acc.set_ylabel('Accuracy')
    ax_acc.legend()
    
    # Save the Accuracy figure
    acc_path = os.path.join(sourceFileDir, "results/validation_plot.png")
    eval_plot.savefig(acc_path)
    
    return eval_plot


import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(correct_labels, predicted_labels):
    class_names = [key.replace('_', ' ').title() for key in type_to_label.keys()]
    confusion_mat = confusion_matrix(correct_labels, predicted_labels)
    
    plt.figure(figsize=(10, 7))
    
    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=class_names, yticklabels=class_names)
    
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    
    return plt
    