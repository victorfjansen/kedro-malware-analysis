from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback
import evaluate
import numpy as np
import matplotlib.pyplot as plt

import csv
import os

 # Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_labels = 2

lang = 'en'

type_to_label = { 
    "non_malware": 0,
    "malware": 1
}

label_to_type = {
    0: "non_malware",
    1: "malware"
}

metric = evaluate.load("accuracy")

class MalwareDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels
    def __len__(self):
        return len(self.sequences['input_ids'])
    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.sequences.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item
    
class MetricsCallback(TrainerCallback):
    def __init__(self):
        self.train_loss = []
        self.eval_loss = []
        self.train_acc = []
        self.eval_acc = []
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            if 'loss' in logs:
                self.train_loss.append(logs['loss'])
            if 'eval_loss' in logs:
                self.eval_loss.append(logs['eval_loss'])
            if 'eval_accuracy' in logs:
                self.eval_acc.append(logs['eval_accuracy'])

  
### Custom callback to evaluate the transformer
  
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
# ---------------------------------------------------------


def split_mnemonic_codes_df(data_frame):
    train_texts, val_texts, train_labels, val_labels, train_filenames, val_filenames = train_test_split(
        data_frame['opcodes'].to_list(), 
        data_frame['type'].map(type_to_label).values, 
        data_frame['file_name'].to_list(), 
        test_size=0.2, 
        random_state=1337,
        shuffle=True
    )
    
    return train_texts, val_texts, train_labels, val_labels, train_filenames, val_filenames



def train_model_with_mnemonic_codes(train_texts, val_texts, train_labels, val_labels):
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    tokenized_train_sequences = tokenizer(train_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    train_dataset = MalwareDataset(tokenized_train_sequences, train_labels)
    
    tokenized_eval_sequences = tokenizer(val_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    eval_dataset = MalwareDataset(tokenized_eval_sequences, val_labels) 
    
    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels).to(device)
    metrics_callback = MetricsCallback()
    
    training_args = TrainingArguments(
        evaluation_strategy="epoch",
        output_dir='./binary_bert_classification',         
        num_train_epochs=5,              
        per_device_train_batch_size=8,  
        per_device_eval_batch_size=8,
        warmup_steps=100,      
        weight_decay=0.1,               
        logging_dir='./logs',            
        logging_steps=2,
    )
    
    trainer = Trainer(
        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,    
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
        callbacks=[metrics_callback]
    )
    
    trainer.train()
    trainer.evaluate()
    return model, metrics_callback

def __classify_the_sequences(sequence, model, tokenizer):
    tokenized_sequences = tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
    outputs = model(**tokenized_sequences)
    
    predictions = torch.argmax(outputs.logits, dim=1)
    malware_prob = torch.sigmoid(outputs.logits)[:, -1].item()
    return predictions, malware_prob

def __get_malware_probability_directory():
    sourceFileDir = os.path.dirname(os.path.abspath(__file__))
    filepath = os.path.join(sourceFileDir, "results/malware_probability.csv")
    return filepath

def __instantiate_malware_probability_header():
    with open(__get_malware_probability_directory(), "w", newline="") as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(["file_name", "mnemonic_opcode", "correct_label", "predicted_label", "malware_prob"]) 

def __save_malware_probability(csvfile, file_name, mnemonic, correct_label, predicted_label, malware_prob):
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow([file_name, mnemonic, correct_label, predicted_label, malware_prob]) 

def evaluate_model_with_mnemonic_codes(model, texts_val, labels_val, filenames_val, shuffled_data):
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("INITIALIZING EVALUATION")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    csv_data = []
    total_correct = 0
    total_test_sets = 0
    accuracy = 0
    
    for idx, mnemonic in enumerate(texts_val):
        prediction, malware_prob =  __classify_the_sequences(mnemonic, model, tokenizer)
        
        correct_label = labels_val[idx]
        print(f"Predicted Label: {prediction.item()}, Correct label: {correct_label}")
        
        total_correct += (prediction.item() == correct_label).sum()
        total_test_sets += 1
        accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
        print(f"trainset accuracy: {accuracy}")
    
    
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("INITIALIZING MALWARE PROBRABILITY CSV CONSTRUCTOR")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    
    
    texts_to_csv = shuffled_data['opcodes'].to_list()
    labels_to_csv = shuffled_data['type'].map(type_to_label).values.tolist()
    filenames_to_csv = shuffled_data['file_name'].to_list()
    
    __instantiate_malware_probability_header()
    with open(__get_malware_probability_directory(), "a", newline="") as csvfile:
        for idx, mnemonic in enumerate(texts_to_csv):
            prediction, malware_prob =  __classify_the_sequences(mnemonic, model, tokenizer)
            
            correct_label = labels_to_csv[idx]
            print(f"Predicted Label: {prediction.item()}, Correct label: {correct_label}")
            
            __save_malware_probability(csvfile, filenames_to_csv[idx], mnemonic, correct_label, prediction.item(), malware_prob)
            
            # Append the row data to csv_data list
            csv_data.append([filenames_to_csv[idx], mnemonic, correct_label, prediction.item(), malware_prob])
            
            total_correct += 1 if (prediction.item() == correct_label) else 0
            total_test_sets += 1
            accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
            print(f"trainset accuracy: {accuracy}")
            
    accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
    print(f"Total accuracy: {accuracy}")
    
    # Create a DataFrame from the collected data
    df = pd.DataFrame(csv_data, columns=["Filename", "Mnemonic", "Correct Label", "Predicted Label", "Malware Probability"])
    
    return accuracy, df.to_csv(index=False)
    
    
import pandas as pd
import plotly.express as px

def plot_metrics_save_figures(metrics_callback):
    os.makedirs('results', exist_ok=True)
    
    # Create a figure for Loss
    fig_loss, ax_loss = plt.subplots(figsize=(12, 5))
    ax_loss.plot(metrics_callback.train_loss, 'bo-', label='Training loss')
    ax_loss.plot(metrics_callback.eval_loss, 'ro-', label='Validation loss')
    ax_loss.set_title('Training and Validation Loss')
    ax_loss.set_xlabel('Epochs')
    ax_loss.set_ylabel('Loss')
    ax_loss.legend()
    
    # Save the Loss figure
    sourceFileDir = os.path.dirname(os.path.abspath(__file__))
    loss_path = os.path.join(sourceFileDir, "results/loss_plot.png")
    fig_loss.savefig(loss_path)
    
    # Create a figure for Accuracy
    fig_acc, ax_acc = plt.subplots(figsize=(12, 5))
    ax_acc.plot(metrics_callback.eval_acc, 'ro-', label='Validation accuracy')
    ax_acc.set_title('Validation Accuracy')
    ax_acc.set_xlabel('Epochs')
    ax_acc.set_ylabel('Accuracy')
    ax_acc.legend()
    
    # Save the Accuracy figure
    acc_path = os.path.join(sourceFileDir, "results/accuracy_plot.png")
    fig_acc.savefig(acc_path)
    
    return fig_loss, fig_acc

def plot_metrics(metrics_callback):
    # Create a DataFrame for Plotly
    df_train = pd.DataFrame({
        'Epoch': list(range(1, len(metrics_callback.train_loss) + 1)),
        'Training Loss': metrics_callback.train_loss,
        'Training Accuracy': [None] * len(metrics_callback.train_loss)  # Placeholder for training accuracy if needed
    })
    
    df_eval = pd.DataFrame({
        'Epoch': list(range(1, len(metrics_callback.eval_loss) + 1)),
        'Validation Loss': metrics_callback.eval_loss,
        'Validation Accuracy': metrics_callback.eval_acc
    })
    
    # Plot Training Metrics with Plotly
    fig_train = px.line(df_train, x='Epoch', y=['Training Loss'], 
                        labels={'value': 'Loss', 'variable': 'Metric Type'},
                        title='Training Metrics (Loss)')
    
    # Plot Evaluation Metrics with Plotly
    fig_eval = px.line(df_eval, x='Epoch', y=['Validation Loss', 'Validation Accuracy'], 
                       labels={'value': 'Metrics', 'variable': 'Metric Type'},
                       title='Evaluation Metrics (Loss and Accuracy)')
    
    return fig_train, fig_eval