from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback
import evaluate
import numpy as np
import matplotlib.pyplot as plt

import wandb

import csv
import os

 # Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_labels = 2

lang = 'en'

type_to_label = { 
    "non_malware": 0,
    "malware": 1
}

label_to_type = {
    0: "non_malware",
    1: "malware"
}

metric = evaluate.load("accuracy")

class MalwareDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels
    def __len__(self):
        return len(self.sequences['input_ids'])
    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.sequences.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item
    
class MetricsCallback(TrainerCallback):
    def __init__(self):
        self.train_loss = []
        self.eval_loss = []
        self.train_acc = []
        self.eval_acc = []
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            print(logs, "AQUIIIIIIIIIIIIIII")
            if 'loss' in logs:
                self.train_loss.append(logs['loss'])
            if 'eval_loss' in logs:
                self.eval_loss.append(logs['eval_loss'])
            if 'eval_accuracy' in logs:
                self.eval_acc.append(logs['eval_accuracy'])
            if 'accuracy' in logs:  # 'accuracy' is logged during training by the Trainer
                self.train_acc.append(logs['accuracy'])
                
class CustomTrainer(Trainer):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.epoch_predictions = []
        self.epoch_labels = []
        self.epoch_loss = []

    def compute_loss(self, model, inputs, return_outputs=False):
        """
        MAX: Subclassed to compute training accuracy.

        How the loss is computed by Trainer. By default, all models return the loss in
        the first element.

        Subclass and override for custom behavior.
        """
        if self.label_smoother is not None and "labels" in inputs:
            labels = inputs.pop("labels")
        else:
            labels = None
        outputs = model(**inputs)

        if "labels" in inputs:
            preds = outputs.logits.detach()

            # Log accuracy
            acc = (
                (preds.argmax(axis=1) == inputs["labels"])
                .type(torch.float)
                .mean()
                .item()
            )
            # Uncomment it if you want to plot the batch accuracy
            # wandb.log({"batch_accuracy": acc})  # Log accuracy

            # Store predictions and labels for epoch-level metrics
            self.epoch_predictions.append(preds.cpu().numpy())
            self.epoch_labels.append(inputs["labels"].cpu().numpy())

        # Save past state if it exists
        if self.args.past_index >= 0:
            self._past = outputs[self.args.past_index]

        if labels is not None:
            loss = self.label_smoother(outputs, labels)
        else:
            loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]
            # Uncomment it if you want to plot the batch loss
            # wandb.log({"batch_loss": loss})
            self.epoch_loss.append(loss.item())  # Store loss for epoch-level metrics

        return (loss, outputs) if return_outputs else loss
    
class CustomCallback(TrainerCallback):

    def __init__(self, trainer) -> None:
        super().__init__()
        self._trainer = trainer

    def on_epoch_end(self, args, state, control, **kwargs):
        # Aggregate predictions and labels for the entire epoch
        epoch_predictions = np.concatenate(self._trainer.epoch_predictions)
        epoch_labels = np.concatenate(self._trainer.epoch_labels)

        # Compute accuracy
        accuracy = np.mean(epoch_predictions.argmax(axis=1) == epoch_labels)

        # Compute mean loss
        mean_loss = np.mean(self._trainer.epoch_loss)

        # Compute precision, recall, and F1-score
        precision, recall, f1, _ = precision_recall_fscore_support(
            epoch_labels, epoch_predictions.argmax(axis=1), average="weighted"
        )

        # Log epoch-level metrics
        wandb.log({"epoch_accuracy": accuracy, "epoch_loss": mean_loss})
        wandb.log({"precision": precision, "recall": recall, "f1": f1})

        # Clear stored predictions, labels, and loss for the next epoch
        self._trainer.epoch_predictions = []
        self._trainer.epoch_labels = []
        self._trainer.epoch_loss = []
        return None

  
### Custom callback to evaluate the transformer
  
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
# ---------------------------------------------------------


def split_mnemonic_codes_df(data_frame):
    train_texts, val_texts, train_labels, val_labels, train_filenames, val_filenames = train_test_split(
        data_frame['opcodes'].to_list(), 
        data_frame['type'].map(type_to_label).values, 
        data_frame['file_name'].to_list(), 
        test_size=0.2, 
        random_state=1337,
        shuffle=True
    )
    
    return train_texts, val_texts, train_labels, val_labels, train_filenames, val_filenames



def train_model_with_mnemonic_codes(train_texts, val_texts, train_labels, val_labels):
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    tokenized_train_sequences = tokenizer(train_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    train_dataset = MalwareDataset(tokenized_train_sequences, train_labels)
    
    tokenized_eval_sequences = tokenizer(val_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    eval_dataset = MalwareDataset(tokenized_eval_sequences, val_labels) 
    
    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels).to(device)
    metrics_callback = MetricsCallback()
    
    training_args = TrainingArguments(
        evaluation_strategy="epoch",
        output_dir='./binary_bert_classification',         
        num_train_epochs=5,              
        per_device_train_batch_size=8,  
        per_device_eval_batch_size=8,
        warmup_steps=100,      
        weight_decay=0.1,               
        logging_dir='./logs',            
        logging_steps=2,
    )
    
    trainer = CustomTrainer(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )
    
    # Trainer(
    #     model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    #     args=training_args,                  # training arguments, defined above
    #     train_dataset=train_dataset,    
    #     eval_dataset=eval_dataset,
    #     compute_metrics=compute_metrics,
    #     callbacks=[metrics_callback]
    # )
    
    trainer.train()
    trainer.evaluate()
    return model, metrics_callback

def __classify_the_sequences(sequence, model, tokenizer):
    tokenized_sequences = tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
    outputs = model(**tokenized_sequences)
    
    predictions = torch.argmax(outputs.logits, dim=1)
    malware_prob = torch.sigmoid(outputs.logits)[:, -1].item()
    return predictions, malware_prob

def __get_malware_probability_directory():
    sourceFileDir = os.path.dirname(os.path.abspath(__file__))
    filepath = os.path.join(sourceFileDir, "results/malware_probability.csv")
    return filepath

def __instantiate_malware_probability_header():
    with open(__get_malware_probability_directory(), "w", newline="") as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(["file_name", "mnemonic_opcode", "correct_label", "predicted_label", "malware_prob"]) 

def __save_malware_probability(csvfile, file_name, mnemonic, correct_label, predicted_label, malware_prob):
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow([file_name, mnemonic, correct_label, predicted_label, malware_prob]) 

def evaluate_model_with_mnemonic_codes(model, texts_val, labels_val, filenames_val, shuffled_data):
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("INITIALIZING EVALUATION")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    csv_data = []
    total_correct = 0
    total_test_sets = 0
    accuracy = 0
    
    for idx, mnemonic in enumerate(texts_val):
        prediction, malware_prob =  __classify_the_sequences(mnemonic, model, tokenizer)
        
        correct_label = labels_val[idx]
        total_correct += (prediction.item() == correct_label).sum()
        total_test_sets += 1
        accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
        print(f"trainset accuracy: {accuracy}")
    
    
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("INITIALIZING MALWARE PROBRABILITY CSV CONSTRUCTOR")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    
    
    texts_to_csv = shuffled_data['opcodes'].to_list()
    labels_to_csv = shuffled_data['type'].map(type_to_label).values.tolist()
    filenames_to_csv = shuffled_data['file_name'].to_list()
    
    correct_labels = []
    predicted_labels = []
    
    __instantiate_malware_probability_header()
    with open(__get_malware_probability_directory(), "a", newline="") as csvfile:
        for idx, mnemonic in enumerate(texts_to_csv):
            prediction, malware_prob =  __classify_the_sequences(mnemonic, model, tokenizer)
            
            correct_label = labels_to_csv[idx]
            
            # append to build confusion_matrix
            correct_labels.append(correct_label)
            predicted_labels.append(prediction.item())            
            
            __save_malware_probability(csvfile, filenames_to_csv[idx], mnemonic, correct_label, prediction.item(), malware_prob)
            
            # Append the row data to csv_data list
            csv_data.append([filenames_to_csv[idx], mnemonic, correct_label, prediction.item(), malware_prob])
            
            total_correct += 1 if (prediction.item() == correct_label) else 0
            total_test_sets += 1
            accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
            print(f"trainset accuracy: {accuracy}")
            
    accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
    print(f"Total accuracy: {accuracy}")
    
    # Create a DataFrame from the collected data
    dataframe = pd.DataFrame(csv_data, columns=["Filename", "Mnemonic", "Correct Label", "Predicted Label", "Malware Probability"])
    
    return accuracy, dataframe, correct_labels, predicted_labels
    
    
import pandas as pd
import plotly.express as px

def plot_metrics_save_figures(metrics_callback):
    os.makedirs('results', exist_ok=True)
    
    # Create a figure for Loss
    fig_loss, ax_loss = plt.subplots(figsize=(12, 5))
    ax_loss.plot(metrics_callback.train_loss, label='Training loss')
    ax_loss.plot(metrics_callback.train_acc, label='Training Accuracy')
    ax_loss.set_title('Training and Validation Loss')
    ax_loss.set_xlabel('Epochs')
    ax_loss.set_ylabel('Loss')
    ax_loss.legend()
    
    # Save the Loss figure
    sourceFileDir = os.path.dirname(os.path.abspath(__file__))
    loss_path = os.path.join(sourceFileDir, "results/loss_plot.png")
    fig_loss.savefig(loss_path)
    
    # Create a figure for Accuracy
    fig_acc, ax_acc = plt.subplots(figsize=(12, 5))
    ax_acc.plot(metrics_callback.eval_acc, label='Validation accuracy')
    ax_acc.plot(metrics_callback.eval_loss, label='Validation loss')
    ax_acc.set_title('Validation Accuracy')
    ax_acc.set_xlabel('Epochs')
    ax_acc.set_ylabel('Accuracy')
    ax_acc.legend()
    
    # Save the Accuracy figure
    acc_path = os.path.join(sourceFileDir, "results/accuracy_plot.png")
    fig_acc.savefig(acc_path)
    
    return fig_loss, fig_acc


import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support

def plot_confusion_matrix(correct_labels, predicted_labels):
    class_names = [key.replace('_', ' ').title() for key in type_to_label.keys()]
    confusion_mat = confusion_matrix(correct_labels, predicted_labels)
    
    plt.figure(figsize=(10, 7))
    
    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=class_names, yticklabels=class_names)
    
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    
    return plt
    