from copy import deepcopy
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import torch
from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, TrainerCallback
import math
import evaluate
import numpy as np
from sklearn.metrics import f1_score, accuracy_score

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BATCH_SIZE = 8

lang = 'en'
type_to_label = { 
    "non_malware": 0,
    "malware": 1,
    "Banking-Malware": 2,  # Was 3
    "Botnets": 3,  # Was 4
    "Browser Hijackers": 4,  # Was 5
    "Email-Worm": 5,  # Was 6
    "Joke": 6,  # Was 7
    "Net-Worm": 7,  # Was 8
    "Pony": 8,  # Was 9
    "Ransomware": 9,  # Was 10
    "RAT": 10,  # Was 11
    "rogues": 11,  # Was 12
    "Spyware": 12,  # Was 13
    "Stealer": 13,  # Was 14
    "Trojan": 14,  # Was 15
    "Virus": 15,  # Was 16
    "Worm": 16  # Was 17
}
num_labels = len(type_to_label)

metric = evaluate.load("accuracy")

class MalwareDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels

    def __len__(self):
        return len(self.sequences['input_ids'])

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.sequences.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Change to long for CrossEntropy
        return item
    
class MetricsCallback(TrainerCallback):
    def __init__(self):
        self.train_loss = []
        self.eval_loss = []
        self.train_acc = []
        self.eval_acc = []
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            if 'loss' in logs:
                self.train_loss.append(logs['loss'])
            if 'eval_loss' in logs:
                self.eval_loss.append(logs['eval_loss'])
            if 'eval_accuracy' in logs:
                self.eval_acc.append(logs['eval_accuracy'])
            if 'accuracy' in logs:  # 'accuracy' is logged during training by the Trainer
                self.train_acc.append(logs['accuracy'])    
    
### Custom callback to evaluate the transformer (updated for multiclass)

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)  # Get the predicted class index (instead of sigmoid for multilabel)
    labels = p.label_ids
    
    print("LABELS", labels)
    print("PREDS", preds)

    # Calculate accuracy and f1 score
    accuracy = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average='micro')  # You can use 'macro' or 'weighted' depending on your needs

    metrics = {
        'accuracy': accuracy,
        'f1': f1,
    }
    return metrics

# ---------------------------------------------------------

def split_mnemonic_codes_df(data_frame):
    # No need for one-hot encoding, return labels as integers
    data_frame['type'] = data_frame['type'].apply(lambda x: type_to_label[x])
    
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        data_frame['opcodes'].to_list(), 
        data_frame['type'].to_list(), 
        test_size=0.2, 
        random_state=1337,
        shuffle=True
    )
    
    return train_texts, val_texts, train_labels, val_labels

def train_model_with_mnemonic_codes(train_texts, val_texts, train_labels, val_labels):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    tokenized_train_sequences = tokenizer(train_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    train_dataset = MalwareDataset(tokenized_train_sequences, train_labels)

    tokenized_eval_sequences = tokenizer(val_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    eval_dataset = MalwareDataset(tokenized_eval_sequences, val_labels)

    # Use the default problem type (no need to specify multi-label classification)
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels).to(device)
    metrics_callback = MetricsCallback()
    
    training_args = TrainingArguments(
        evaluation_strategy="epoch",
        save_strategy="epoch",
        output_dir="./bert_multiclass_classification",
        num_train_epochs=5,  
        learning_rate=1e-5,            
        per_device_train_batch_size=BATCH_SIZE,  
        per_device_eval_batch_size=BATCH_SIZE,
        weight_decay=0.01,   
        load_best_model_at_end=True,    
        logging_steps=2,
    )

    trainer = Trainer(
        model=model,                         
        args=training_args,                  
        train_dataset=train_dataset,    
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[metrics_callback]
    )

    trainer.train()
    trainer.evaluate()

    return model, metrics_callback

def __classify_the_sequences(sequence, model, tokenizer):
    tokenized_sequences = tokenizer(sequence, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
    outputs = model(**tokenized_sequences)
    predictions = torch.argmax(outputs.logits, dim=1)  # Get the predicted class index
    return predictions.cpu().numpy()

def evaluate_model_with_mnemonic_codes(model, train_texts, labels_val):
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("INITIALIZING EVALUATION")
    print("--------------------------------------------------")        
    print("--------------------------------------------------")        
    print("--------------------------------------------------")  
    
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    total_correct = 0
    total_test_sets = 0
    
    correct_labels = []
    predicted_labels = []

    for idx, val in enumerate(train_texts):
        prediction = __classify_the_sequences(val, model, tokenizer)
        
        correct_label = labels_val[idx]
        correct_labels.append(correct_label)
        
        predicted_labels.append(prediction)
        
        # Calculate the accuracy per sequence
        total_correct += (prediction == labels_val[idx])
        total_test_sets += 1

    accuracy = total_correct / total_test_sets if total_test_sets > 0 else 0.0
    print(f"Total accuracy: {accuracy}")
    
    return correct_labels, predicted_labels
    
    
import os
import matplotlib.pyplot as plt
    
def plot_metrics_save_figures(metrics_callback):
    os.makedirs('results', exist_ok=True)
    sourceFileDir = os.path.dirname(os.path.abspath(__file__))
    
    # Create a figure for Accuracy
    eval_plot, ax_acc = plt.subplots(figsize=(12, 5))
    ax_acc.plot(metrics_callback.eval_acc, label='Validation accuracy')
    ax_acc.plot(metrics_callback.eval_loss, label='Validation loss')
    ax_acc.set_title('Validation Accuracy')
    ax_acc.set_xlabel('Epochs')
    ax_acc.set_ylabel('Accuracy')
    ax_acc.legend()
    
    # Save the Accuracy figure
    acc_path = os.path.join(sourceFileDir, "results/multilabel_bert_eval_metrics.png")
    eval_plot.savefig(acc_path)
    
    return eval_plot

import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(correct_labels, predicted_labels):
    class_names = [key.replace('_', ' ').title() for key in type_to_label.keys()]
    confusion_mat = confusion_matrix(correct_labels, predicted_labels)
    
    plt.figure(figsize=(10, 7))
    
    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=class_names, yticklabels=class_names)
    
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    
    return plt
